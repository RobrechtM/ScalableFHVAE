Using Python from /esat/spchdisk/scratch/r0797363/venv/ScalableFHVAE/bin/python
PYTHONPATH=/esat/spchdisk/scratch/r0797363/src/ScalableFHVAE:/esat/spchdisk/scratch/r0797363/src/ScalableFHVAE/kaldi_python/kaldi-python:/users/spraak/spch/prog/spch/tensorflow-1.0.1/lib/python2.7/site-packages:/users/spraak/spch/.local/lib/python2.7/site-packages
LD_LIBRARY_PATH=/users/spraak/spch/prog/spch/cudnn-5.1/liblinux-x86_64/:/users/spraak/spch/prog/spch/cuda-8.0/lib64:
Mon Dec  9 10:21:23 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.116                Driver Version: 390.116                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 00000000:05:00.0 Off |                  N/A |
| 24%   36C    P8    17W / 250W |     10MiB / 12196MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
CUDA_VISIBLE_DEVICES=0
I am process 12, running on spchcl23.esat.kuleuven.be: starting (Mon Dec  9 10:21:32 2019)
load arguments from /esat/spchtemp/scratch/hvanhamm/fhvae_timit/exp/cgn_per_speaker/nogau_reg_fhvae_e99_s5000_p10_a10.0_b10.0_c10.0_e0.01/args.pkl
Namespace(adam_eps=0.01, alpha_dis=10.0, alpha_z1=10.0, alpha_z2=10.0, dataset='cgn_per_speaker', fac_root='misc/cgn_per_spk_afgklno_all_talab_%s.scp', facs='reg1:gender', is_numpy=True, model='reg_fhvae', n_epochs=99, n_patience=10, n_print_steps=200, n_steps_per_epoch=5000, talab_facs='pho', talab_root='misc/cgn_per_spk_afgklno_all_talab', tr_nseqs=696, tr_shape=(20, 80), y_fac='spk')
NumpyDataset: 23143 out of 23144 kept, min_len = 20
model trained with normal training, nmu2=696

Factorized Hierarchical Variational Autoencoder:
  Priors (mean/logvar):
    pz1: [0.0, 0.0]
    pz2: [<tf.Tensor 'fhvae/mu2/mu2:0' shape=(?, 32) dtype=float32>, -1.3862944]
    pmu2: [0.0, 0.0]
  Observed Variables:
    xin: Tensor("xin:0", shape=(?, 20, 80), dtype=float32)
    xout: Tensor("xout:0", shape=(?, 20, 80), dtype=float32)
    y: Tensor("y:0", shape=(?,), dtype=int64)
    n: Tensor("n:0", shape=(?,), dtype=float32)
    cReg: Tensor("cReg:0", shape=(?, 2), dtype=int64)
  Encoder/Decoder Architectures:
    z1 encoder:
      LSTM hidden units: [256, 256]
      latent dim: 32
    z2 encoder:
      LSTM hidden units: [256, 256]
      latent dim: 32
    mu2 table size: 696
    x decoder:
      LSTM hidden units: [256, 256]
  Outputs:
    qz1_x: [<tf.Tensor 'fhvae/z1_enc_gauss/mu/BiasAdd:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'fhvae/z1_enc_gauss/logvar/BiasAdd:0' shape=(?, 32) dtype=float32>]
    qz2_x: [<tf.Tensor 'fhvae/z2_enc_gauss/mu/BiasAdd:0' shape=(?, 32) dtype=float32>, <tf.Tensor 'fhvae/z2_enc_gauss/logvar/BiasAdd:0' shape=(?, 32) dtype=float32>]
    mu2: Tensor("fhvae/mu2/mu2:0", shape=(?, 32), dtype=float32)
    px_z: [<tf.Tensor 'fhvae/dec_x_mu:0' shape=(?, 20, 80) dtype=float32>, <tf.Tensor 'fhvae/dec_x_logvar:0' shape=(?, 20, 80) dtype=float32>]
    z1_sample: Tensor("fhvae/z1_enc_gauss/add:0", shape=(?, 32), dtype=float32)
    z2_sample: Tensor("fhvae/z2_enc_gauss/add:0", shape=(?, 32), dtype=float32)
    x_sample: Tensor("fhvae/dec_x_sample:0", shape=(?, 20, 80), dtype=float32)
  Losses:
    lb: Tensor("add_9:0", shape=(?,), dtype=float32)
    log_px_z: Tensor("Sum_3:0", shape=(?,), dtype=float32)
    neg_kld_z1: Tensor("mul_4:0", shape=(?,), dtype=float32)
    neg_kld_z2: Tensor("mul_2:0", shape=(?,), dtype=float32)
    log_pmu2: Tensor("Sum:0", shape=(?,), dtype=float32)
    log_qy: Tensor("Neg:0", shape=(?,), dtype=float32)
  Parameters:
    fhvae/mu2/mu2_table:0, (696, 32)
    fhvae/z2_enc_lstm_256_256/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0, (336, 1024)
    fhvae/z2_enc_lstm_256_256/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0, (1024,)
    fhvae/z2_enc_lstm_256_256/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0, (512, 1024)
    fhvae/z2_enc_lstm_256_256/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0, (1024,)
    fhvae/z2_enc_gauss/mu/weights:0, (512, 32)
    fhvae/z2_enc_gauss/mu/biases:0, (32,)
    fhvae/z2_enc_gauss/logvar/weights:0, (512, 32)
    fhvae/z2_enc_gauss/logvar/biases:0, (32,)
    fhvae/z1_enc_lstm_256_256/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0, (368, 1024)
    fhvae/z1_enc_lstm_256_256/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0, (1024,)
    fhvae/z1_enc_lstm_256_256/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0, (512, 1024)
    fhvae/z1_enc_lstm_256_256/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0, (1024,)
    fhvae/z1_enc_gauss/mu/weights:0, (512, 32)
    fhvae/z1_enc_gauss/mu/biases:0, (32,)
    fhvae/z1_enc_gauss/logvar/weights:0, (512, 32)
    fhvae/z1_enc_gauss/logvar/biases:0, (32,)
    fhvae/dec_lstm_256_256_step/cell_0/basic_lstm_cell/weights:0, (320, 1024)
    fhvae/dec_lstm_256_256_step/cell_0/basic_lstm_cell/biases:0, (1024,)
    fhvae/dec_lstm_256_256_step/cell_1/basic_lstm_cell/weights:0, (512, 1024)
    fhvae/dec_lstm_256_256_step/cell_1/basic_lstm_cell/biases:0, (1024,)
    fhvae/dec_gauss_step/mu/weights:0, (256, 80)
    fhvae/dec_gauss_step/mu/biases:0, (80,)
    fhvae/dec_gauss_step/logvar/weights:0, (256, 80)
    fhvae/dec_gauss_step/logvar/biases:0, (80,)
    fhvae/reg_pho/weights:0, (32, 47)
    fhvae/reg_pho/biases:0, (47,)
    fhvae/reg_reg1/weights:0, (32, 4)
    fhvae/reg_reg1/biases:0, (4,)
    fhvae/reg_gender/weights:0, (32, 2)
    fhvae/reg_gender/biases:0, (2,)

Previous Progress:
[epoch    step  pass  best best_lb   time]
[    1    5000     1     1 -682.51    682] -682.51 -591.54 -78.85 -11.78 -30.94
[    2   10000     1     2 -603.93   1092] -603.93 -503.16 -79.25 -21.17 -31.90
[    3   15000     1     3 -546.75   1503] -546.75 -437.43 -78.30 -30.66 -32.24
[    4   20000     1     4 -510.99   1916] -510.99 -396.00 -77.80 -36.83 -32.25
[    5   25000     2     5 -491.41   2674] -491.41 -373.38 -78.02 -39.66 -32.29
[    6   30000     2     6 -480.58   3086] -480.58 -361.84 -76.84 -41.55 -32.29
[    7   35000     2     7 -470.97   3500] -470.97 -351.73 -76.64 -42.24 -32.34
[    8   40000     2     8 -463.87   3909] -463.87 -344.20 -76.54 -42.77 -32.30
[    9   45000     3     9 -460.33   4764] -460.33 -340.00 -76.76 -43.22 -32.38
[   10   50000     3    10 -456.45   5173] -456.45 -336.30 -76.29 -43.51 -32.40
[   11   55000     3    10 -456.45   5587] -465.46 -345.42 -76.30 -43.38 -32.37
[   12   60000     3    12 -450.25   6003] -450.25 -329.87 -75.98 -44.05 -32.45
[   13   65000     4    13 -447.92   7129] -447.92 -327.22 -76.15 -44.19 -32.43
[   14   70000     4    13 -447.92   7542] -461.37 -341.34 -75.76 -43.91 -32.39
[   15   75000     4    13 -447.92   7962] -451.69 -331.29 -75.81 -44.24 -32.46
[   16   80000     4    13 -447.92   8384] -450.86 -330.32 -76.30 -43.88 -32.43
[   17   85000     5    17 -447.42   9521] -447.42 -327.46 -75.72 -43.87 -32.49
[   18   90000     5    18 -433.01   9940] -433.01 -311.37 -76.57 -44.71 -32.46
[   19   95000     5    19 -432.83  10362] -432.83 -311.33 -76.62 -44.52 -32.57
[   20  100000     5    20 -431.60  10785] -431.60 -309.93 -76.67 -44.64 -32.50
[   21  105000     6    20 -431.60  11971] -437.97 -316.87 -76.09 -44.65 -32.48
[   22  110000     6    22 -427.25  12386] -427.25 -305.37 -76.50 -45.03 -32.46
[   23  115000     6    22 -427.25  12811] -431.64 -309.65 -76.50 -45.13 -32.54
[   24  120000     6    24 -426.00  13240] -426.00 -304.32 -76.44 -44.89 -32.58
[   25  125000     7    24 -426.00  14477] -426.59 -304.90 -76.42 -44.92 -32.48
[   26  130000     7    26 -425.98  14882] -425.98 -304.26 -76.37 -45.00 -32.49
[   27  135000     7    26 -425.98  15301] -437.36 -316.02 -75.98 -45.00 -32.48
[   28  140000     7    26 -425.98  15724] -428.63 -306.57 -76.61 -45.09 -32.48
[   29  145000     8    29 -421.27  16897] -421.27 -299.31 -76.49 -45.10 -32.48
[   30  150000     8    30 -418.86  17311] -418.86 -296.41 -76.75 -45.34 -32.48
[   31  155000     8    30 -418.86  17733] -419.60 -297.65 -76.62 -44.98 -32.51
[   32  160000     8    30 -418.86  18154] -419.35 -297.01 -76.75 -45.23 -32.58
[   33  165000     9    33 -418.36  19354] -418.36 -295.81 -76.81 -45.39 -32.50
[   34  170000     9    34 -415.06  19763] -415.06 -291.95 -76.97 -45.79 -32.54
[   35  175000     9    34 -415.06  20190] -421.85 -299.33 -76.71 -45.45 -32.52
[   36  180000     9    36 -413.01  20615] -413.01 -290.41 -76.88 -45.36 -32.52
[   37  185000    10    36 -413.01  21798] -414.49 -291.47 -77.07 -45.60 -32.50
[   38  190000    10    36 -413.01  22201] -414.71 -291.99 -76.80 -45.56 -32.56
[   39  195000    10    39 -412.59  22619] -412.59 -289.91 -76.77 -45.56 -32.53
[   40  200000    10    40 -410.53  23040] -410.53 -287.47 -76.86 -45.84 -32.52
[   41  205000    11    41 -410.41  24257] -410.41 -286.93 -77.31 -45.81 -32.56
[   42  210000    11    41 -410.41  24662] -411.20 -288.44 -76.80 -45.60 -32.52
[   43  215000    11    41 -410.41  25084] -415.84 -292.76 -76.96 -45.76 -32.53
[   44  220000    11    41 -410.41  25505] -414.77 -291.74 -77.02 -45.65 -32.57
[   45  225000    12    45 -409.57  26729] -409.57 -286.06 -77.23 -45.92 -32.51
[   46  230000    12    45 -409.57  27132] -411.92 -288.55 -77.23 -45.79 -32.51
[   47  235000    12    47 -407.87  27555] -407.87 -284.63 -77.11 -45.78 -32.55
[   48  240000    12    47 -407.87  27980] -422.95 -300.20 -76.80 -45.60 -32.55
[   49  245000    12    49 -407.35  28408] -407.35 -284.23 -76.83 -45.94 -32.54
[   50  250000    13    49 -407.35  29611] -408.36 -284.53 -77.41 -46.06 -32.56
[   51  255000    13    51 -406.68  30023] -406.68 -283.42 -76.95 -45.95 -32.58
[   52  260000    13    51 -406.68  30446] -408.10 -284.88 -76.87 -45.99 -32.55
[   53  265000    13    51 -406.68  30872] -419.96 -296.98 -76.79 -45.85 -32.56
[   54  270000    14    51 -406.68  32076] -415.31 -291.90 -77.04 -46.02 -32.55
[   55  275000    14    55 -405.71  32496] -405.71 -282.14 -77.05 -46.16 -32.51
[   56  280000    14    56 -404.58  32915] -404.58 -280.98 -77.23 -46.00 -32.51
[   57  285000    14    56 -404.58  33340] -405.70 -281.73 -77.26 -46.36 -32.53
[   58  290000    15    56 -404.58  34537] -408.59 -285.25 -76.95 -46.04 -32.56
[   59  295000    15    56 -404.58  34955] -404.59 -280.69 -77.24 -46.30 -32.51
[   60  300000    15    60 -402.76  35379] -402.76 -278.79 -77.29 -46.32 -32.56
[   61  305000    15    60 -402.76  35804] -403.21 -279.16 -77.34 -46.36 -32.53
[   62  310000    16    60 -402.76  36980] -404.93 -281.01 -76.99 -46.58 -32.59
[   63  315000    16    60 -402.76  37391] -407.34 -283.93 -76.93 -46.13 -32.65
[   64  320000    16    64 -402.75  37810] -402.75 -279.23 -77.12 -46.04 -32.57
[   65  325000    16    65 -401.45  38235] -401.45 -277.27 -77.29 -46.53 -32.60
[   66  330000    17    65 -401.45  39444] -403.74 -279.38 -77.41 -46.60 -32.57
[   67  335000    17    65 -401.45  39857] -405.84 -282.09 -77.11 -46.28 -32.61
[   68  340000    17    65 -401.45  40280] -405.70 -281.88 -77.19 -46.27 -32.60
[   69  345000    17    69 -401.20  40701] -401.20 -277.15 -77.31 -46.38 -32.59
[   70  350000    18    69 -401.20  42009] -402.43 -278.73 -77.02 -46.33 -32.59
[   71  355000    18    69 -401.20  42420] -401.60 -277.14 -77.35 -46.75 -32.58
[   72  360000    18    69 -401.20  42844] -407.47 -283.20 -77.32 -46.59 -32.59
[   73  365000    18    69 -401.20  43273] -402.77 -278.55 -77.37 -46.49 -32.63
[   74  370000    19    69 -401.20  44445] -636.00 -520.72 -74.71 -40.22 -32.65
[   75  375000    19    69 -401.20  44853] -493.01 -375.85 -76.32 -40.49 -32.01
[   76  380000    19    69 -401.20  45273] -449.99 -328.49 -76.85 -44.29 -32.23
[   77  385000    19    69 -401.20  45698] -409.18 -285.29 -77.76 -45.78 -32.43
[   78  390000    20    69 -401.20  47050] -407.84 -284.13 -77.46 -45.89 -32.50
[   79  395000    20    69 -401.20  47463] -2097.17 -1678.51 -136.47 -280.10 -190.67
[   80  400000    20    69 -401.20  47881] -1652.77 -1389.82 -84.68 -175.56 -245.03

Best Epoch Statistics:
[   69  345000    17    69 -401.20  40701]
reading parameters from /esat/spchtemp/scratch/hvanhamm/fhvae_timit/exp/cgn_per_speaker/nogau_reg_fhvae_e99_s5000_p10_a10.0_b10.0_c10.0_e0.01/models/fhvae-345000
restore model takes 99.64 s
########################################
avg. norm = 2.73, #mu2 = 23143
per dim: 0.23 0.27 0.35 0.63 0.62 0.39 0.16 0.32 0.25 0.25 0.32 0.25 0.38 0.12 0.54 0.60 0.53 0.40 0.32 0.54 0.37 0.69 0.29 0.32 0.27 0.10 0.22 0.28 0.38 0.62 0.37 0.09
validation takes 8162s
lb=-404.49 log_px_z=-281.20 neg_kld_z1=-77.14 neg_kld_z2=-45.78 log_pmu2=-33.26
########################################
reading parameters from /esat/spchtemp/scratch/hvanhamm/fhvae_timit/exp/cgn_per_speaker/nogau_reg_fhvae_e99_s5000_p10_a10.0_b10.0_c10.0_e0.01/models/fhvae-345000
restore model takes 1.16 s
